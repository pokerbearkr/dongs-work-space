자연어 처리 기본기
==
## 자연어 처리란?
언어는 크게 인공어와 자연어로 나눌 수 있다.
컴퓨터는 인공어로만 소통하기 때문에 자연어를 다루려면 별도의 처리과정이 필요하다.
그 과정을 자연어 처리(NLP:Natural Language Procressing)이라고 한다.

## 자연어 전처리
데이터를 사용하기전에 전처리를 어떻게 하냐에 따라서 분석결과가 크게 달라진다.

자연어 전처리에는 4가지 과정이 있다.
1. 토큰화 : 자연어 데이터를 분석을 위한 작은 단위로 분리
2. 정제 : 분석에 큰 의미가 없는 데이터들을 제거
3. 정규화 : 방법이 다르지만 의미가 같은 데이터들을 통합
4. 정수 인코딩 : 컴퓨터가 이해하기 편하도록 자연어 데이터에 정수 인덱스를 부여

## 단어 토큰화
분석에 활용하기 위한 자연어 데이터를 코퍼스(Corpus)라고 한다.
영어 자연어 처리에는 NLTK라는 패키지가 사용된다.

```Python3
conda install nltk
from nltk.tokenize import word_tokenize

text = "Although it's not a happily-ever-after ending, it is very realistic."

import nltk

nltk.download('punkt')
# 단어 토큰화
tokenized_words = word_tokenize(text)

print(tokenized_words)

```

## 정제
아무 의미도 없거나 목적에 부합하지 않는 단어들을 제거하는 과정이다.
그중에서도 등장 빈도, 단어 길이, 불용어 등을 기준으로 잡는다.

일단 등장 빈도로 정제를 하는코드는
```# 전체 단어 토큰 리스트
tokenized_words = word_tokenize(corpus)

# 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성
vocab = Counter(tokenized_words)

# 빈도수가 2 이하인 단어 리스트 추출
uncommon_words = [key for key, value in vocab.items() if value <= 2]

# 빈도수가 2 이하인 단어들만 제거한 결과를 따로 저장
cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]
```

카운터 함수를 사용하여 단어의 빈도수를 카운트하여 집할을 만든 후 for문 사용하여 저장한다.

다음은 단어의 길이로 정제를 하는 함수이다.
```
def clean_by_len(tokenized_words, cut_off_length):
    # 길이가 cut_off_length 이하인 단어 제거
    cleaned_by_freq_len = []
    
    for word in tokenized_words:
        if len(word) > cut_off_length:
            cleaned_by_freq_len.append(word)

    return cleaned_by_freq_len
```

## 불용어
코퍼스에서 큰 의미가 없거나, 분석 목적에서 벗어나는 단어들을 불용어라고 한다.
불용어들은 정확한 분석을 방해하기 때문에 제거해야 한다.

NLTK는 기본적으로 179개의 불용어를 제공한다. 다음은 NLTK에서 제공하는 불용어를 가져오는 코드이다.
```
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords_set = set(stopwords.words('english'))

print('불용어 개수 :', len(stopwords_set))
print(stopwords_set)
```

만약 불용어들을 추가하거나 빼고싶으면
```
stopwords_set.add('hello')
stopwords_set.remove('the')
```
이렇게 추가하거나 빼면 된다.

이제 불러온 불용어를 이용해서
```
cleaned_words = []

for word in cleaned_by_freq_len:
    if word not in stopwords_set:
        cleaned_words.append(word)
```
명령어로 불용어를 제거하면 된다.

## 정규화
보편적인 정규화 방법 2가지만 다뤄보겠다.
1. 대소문자 통합 : 내장함수인 lower()함수를 사용해서 정규화한다.
2. 규칙 기반 정규화 : 동의어사전 같은것을 이용해서 동의어들을 한가지로 축약해준다.


## 어간추출
단어의 핵심이 되는 부분을 어간이라고 한다.
NLTk는 어간 추출 알고리즘을 제공한다.
어간추출 코드는 다음과 같다
```
from nltk.stem import PorterStemmer

porter_stemmer = PorterStemmer()
text = "You are so lovely. I am loving you now."
porter_stemmed_words = []

# 단어 토큰화
tokenized_words = nltk.word_tokenize(text)

# 포터 스테머의 어간 추출
for word in tokenized_words:
    stem = porter_stemmer.stem(word)
    porter_stemmed_words.append(stem)
```

## 문장토큰화
가끔은 문장단위로 토큰화 한 다음 문장의 의미를 살려서 분석해야 할 경우가 있다. 그럴때는 다음과 같은 코드를 쓰면 된다.
```
from nltk.tokenize import sent_tokenize
nltk.download('punkt')
text = "My email address is 'abcde@codeit.com'. Send it to Mr.Kim."

# 문장 토큰화
tokenized_sents = sent_tokenize(text)
```
## 품사 태깅
단어의 의미를 제대로 파악하려면 문장안에서 단어가 어떤 품사로 사용되었는지 알아야 한다.
```
from nltk.tag import pos_tag

text = "Watching Time Chasers, it obvious that it was made by a bunch of friends. Maybe they were sitting around one day in film school and said, \"Hey, let\'s pool our money together and make a really bad movie!\" Or something like that."
pos_tagged_words = []

# 문장 토큰화
tokenized_sents = sent_tokenize(text)

for sentence in tokenized_sents:
    # 단어 토큰화
    tokenized_words = word_tokenize(sentence)

    # 품사 태깅
    pos_tagged = pos_tag(tokenized_words)
    pos_tagged_words.extend(pos_tagged)
```
