자연어 처리 기본기
==
## 자연어 처리란?
언어는 크게 인공어와 자연어로 나눌 수 있다.
컴퓨터는 인공어로만 소통하기 때문에 자연어를 다루려면 별도의 처리과정이 필요하다.
그 과정을 자연어 처리(NLP:Natural Language Procressing)이라고 한다.

## 자연어 전처리
데이터를 사용하기전에 전처리를 어떻게 하냐에 따라서 분석결과가 크게 달라진다.

자연어 전처리에는 4가지 과정이 있다.
1. 토큰화 : 자연어 데이터를 분석을 위한 작은 단위로 분리
2. 정제 : 분석에 큰 의미가 없는 데이터들을 제거
3. 정규화 : 방법이 다르지만 의미가 같은 데이터들을 통합
4. 정수 인코딩 : 컴퓨터가 이해하기 편하도록 자연어 데이터에 정수 인덱스를 부여

## 단어 토큰화
분석에 활용하기 위한 자연어 데이터를 코퍼스(Corpus)라고 한다.
영어 자연어 처리에는 NLTK라는 패키지가 사용된다.

```Python3
conda install nltk
from nltk.tokenize import word_tokenize

text = "Although it's not a happily-ever-after ending, it is very realistic."

import nltk

nltk.download('punkt')
# 단어 토큰화
tokenized_words = word_tokenize(text)

print(tokenized_words)

```

## 정제
아무 의미도 없거나 목적에 부합하지 않는 단어들을 제거하는 과정이다.
그중에서도 등장 빈도, 단어 길이, 불용어 등을 기준으로 잡는다.

일단 등장 빈도로 정제를 하는코드는
```# 전체 단어 토큰 리스트
tokenized_words = word_tokenize(corpus)

# 파이썬의 Counter 모듈을 통해 단어의 빈도수 카운트하여 단어 집합 생성
vocab = Counter(tokenized_words)

# 빈도수가 2 이하인 단어 리스트 추출
uncommon_words = [key for key, value in vocab.items() if value <= 2]

# 빈도수가 2 이하인 단어들만 제거한 결과를 따로 저장
cleaned_by_freq = [word for word in tokenized_words if word not in uncommon_words]
```

카운터 함수를 사용하여 단어의 빈도수를 카운트하여 집할을 만든 후 for문 사용하여 저장한다.

다음은 단어의 길이로 정제를 하는 함수이다.
```
def clean_by_len(tokenized_words, cut_off_length):
    # 길이가 cut_off_length 이하인 단어 제거
    cleaned_by_freq_len = []
    
    for word in tokenized_words:
        if len(word) > cut_off_length:
            cleaned_by_freq_len.append(word)

    return cleaned_by_freq_len
```

## 불용어
코퍼스에서 큰 의미가 없거나, 분석 목적에서 벗어나는 단어들을 불용어라고 한다.
불용어들은 정확한 분석을 방해하기 때문에 제거해야 한다.

NLTK는 기본적으로 179개의 불용어를 제공한다. 다음은 NLTK에서 제공하는 불용어를 가져오는 코드이다.
```
from nltk.corpus import stopwords
nltk.download('stopwords')
stopwords_set = set(stopwords.words('english'))

print('불용어 개수 :', len(stopwords_set))
print(stopwords_set)
```

만약 불용어들을 추가하거나 빼고싶으면
```
stopwords_set.add('hello')
stopwords_set.remove('the')
```
이렇게 추가하거나 빼면 된다.

이제 불러온 불용어를 이용해서
```
cleaned_words = []

for word in cleaned_by_freq_len:
    if word not in stopwords_set:
        cleaned_words.append(word)
```
명령어로 불용어를 제거하면 된다.

## 정규화
보편적인 정규화 방법 2가지만 다뤄보겠다.
1. 대소문자 통합 : 내장함수인 lower()함수를 사용해서 정규화한다.
2. 규칙 기반 정규화 : 동의어사전 같은것을 이용해서 동의어들을 한가지로 축약해준다.


## 어간추출

